{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b246d9eb",
   "metadata": {},
   "source": [
    "# Face ID Detection With Custom Data\n",
    "\n",
    "**References Source: https://github.com/mdsarfarazulh/face-verification-facenet-mtcnn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93089b94",
   "metadata": {},
   "source": [
    "### Install necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mtcnn\n",
    "# Note! Skiping step when having this available package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba695f",
   "metadata": {},
   "source": [
    "### Importing necessaty librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a219efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using the vgg16 model as a feature extraction model\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from pickle import dump\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np \n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import pickle\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8486d",
   "metadata": {},
   "source": [
    "### Preprocessing image and Extracting feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc3e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproces_image(path):\n",
    "    \n",
    "  # load an image from file\n",
    "    image = load_img(path, target_size=(224, 224))\n",
    "    image = image.resize((224,224))\n",
    "  # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "  # reshape data for the model\n",
    "  #image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = np.expand_dims(image, axis = 0)\n",
    "  # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "def feature_extract(model, image):\n",
    "    features = model.predict(image)[0]\n",
    "  # Vector normalization\n",
    "    features = features / np.linalg.norm(features)\n",
    "    return features\n",
    "\n",
    "### This case, we you VGG16 model pre-train to extracting features.\n",
    "\n",
    "# load model\n",
    "model = VGG16()\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77277e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MTCNN model as detector to detecting face in each frame\n",
    "detector = MTCNN()\n",
    "\n",
    "# Upload file contain vector of face previous extracted\n",
    "p_file = \"vectors.pkl\"\n",
    "\n",
    "# Set some parameters\n",
    "font = cv2.FONT_HERSHEY_DUPLEX\n",
    "color = (0, 255, 255)\n",
    "line = cv2.LINE_AA\n",
    "\n",
    "# Setting the threshold value.\n",
    "threshold = 15.0\n",
    "\n",
    "def get_euclidean(X, Y):\n",
    "    return np.sqrt(np.sum(np.square(np.subtract(X, Y))))\n",
    "\n",
    "with open(p_file, 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "print('Loading data!')\n",
    "\n",
    "# Label of each face in list vector, sort by order\n",
    "Y = [\"Quan\", \"Phuong\", \"Truong\", \"Thuan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78afafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening a video source either file or webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Checking whether video source opened or not.\n",
    "if not cap.isOpened():\n",
    "    print('Video not opened!')\n",
    "\n",
    "\n",
    "# Operating until video source is present.\n",
    "while cap.isOpened():\n",
    "\n",
    "    _, frame  = cap.read()\n",
    "    ##############################\n",
    "\n",
    "    faces = detector.detect_faces(frame)\n",
    "    \n",
    "    for face in faces:\n",
    "        # Getting the co-ordinates of the bounding box.\n",
    "        x, y, w, h = face['box']\n",
    "        # Getting Region Of Image(ROI)\n",
    "        f_img = frame[y:y+h, x:x+w]\n",
    "        # Resizing the face in the shape of (self.width, self.height)\n",
    "        f_img = cv2.resize(f_img, (224, 224))\n",
    "        # Calling the helper function to get the label.\n",
    "        \n",
    "        ##################################\n",
    "        f_img = f_img.astype(np.float16).reshape((1, 224, 224, 3))\n",
    "        # Normalizing the data to reduce computing time and memory.\n",
    "        f_img /= 255.0\n",
    "        \n",
    "        feature = feature_extract(model, f_img)\n",
    "        dist = []\n",
    "        \n",
    "        # Calculating euclidean distance.\n",
    "        for vector in data_dict:\n",
    "            dist.append(get_euclidean(vector, feature))\n",
    "        \n",
    "        print(dist)\n",
    "        dist = np.array(dist)\n",
    "        # Getting the most similar face.\n",
    "        indx = np.argmin(dist)\n",
    "        \n",
    "        if dist[indx] < threshold:\n",
    "            label = Y[indx]\n",
    "        else:\n",
    "            label =  \"Opps!\" \n",
    "        ###################################\n",
    "        # Drawing rectangle and putting text on the bounding box of each fce\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), color, 2, line)\n",
    "        cv2.putText(frame, label, (x-3, y-3), font, 0.5, color, 1)\n",
    "    \n",
    "    ##############################\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27 or k == ord('q'):\n",
    "        break\n",
    "            \n",
    "# Releasing the video source.\n",
    "cap.release()\n",
    "# Destroying all the windows utilised by the app.\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
